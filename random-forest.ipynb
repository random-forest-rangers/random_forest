{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b02523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:48:15.819455Z",
     "iopub.status.busy": "2025-10-20T00:48:15.819136Z",
     "iopub.status.idle": "2025-10-20T00:48:26.743504Z",
     "shell.execute_reply": "2025-10-20T00:48:26.742315Z",
     "shell.execute_reply.started": "2025-10-20T00:48:15.819431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error, mean_tweedie_deviance\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f3c1f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:56:31.207903Z",
     "iopub.status.busy": "2025-10-20T01:56:31.207574Z",
     "iopub.status.idle": "2025-10-20T01:56:31.216233Z",
     "shell.execute_reply": "2025-10-20T01:56:31.214683Z",
     "shell.execute_reply.started": "2025-10-20T01:56:31.207881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n",
    "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction\")\n",
    "    CATBOOST_MODEL_PATH = \"Fall2025/AML/Project/models/catboost_5fold_models.pkl\"\n",
    "    LSTM_MODEL_DIR = \"Fall2025/AML/Project/models/output\"\n",
    "    \n",
    "    ENSEMBLE_WEIGHTS = {\n",
    "        'catboost': 0.5,\n",
    "        'lstm': 0.5\n",
    "    }\n",
    "    \n",
    "    ROLE_SPECIFIC_WEIGHTS = {\n",
    "        'Passer': {'catboost': 0.6, 'lstm': 0.4},\n",
    "        'Targeted Receiver': {'catboost': 0.4, 'lstm': 0.6},\n",
    "        'Defensive Coverage': {'catboost': 0.45, 'lstm': 0.55},\n",
    "        'default': {'catboost': 0.5, 'lstm': 0.5}\n",
    "    }\n",
    "    \n",
    "    USE_ROLE_SPECIFIC_WEIGHTS = False\n",
    "    \n",
    "    LSTM_N_FOLDS = 5\n",
    "    LSTM_WINDOW_SIZE = 8\n",
    "    \n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2174b442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:48:26.754653Z",
     "iopub.status.busy": "2025-10-20T00:48:26.754344Z",
     "iopub.status.idle": "2025-10-20T00:48:51.488874Z",
     "shell.execute_reply": "2025-10-20T00:48:51.487752Z",
     "shell.execute_reply.started": "2025-10-20T00:48:26.754619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input files...\n",
      "Loaded 18 input files.\n",
      "\n",
      "Loading output files...\n",
      "Loaded 18 output files.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading input files...\")\n",
    "input_files = glob.glob(os.path.join(Config.BASE_DIR, 'train/input_*.csv'))\n",
    "input_df = pd.concat([pd.read_csv(f) for f in input_files], ignore_index=True)\n",
    "print(f\"Loaded {len(input_files)} input files.\")\n",
    "\n",
    "print(\"\\nLoading output files...\")\n",
    "output_files = glob.glob(os.path.join(Config.BASE_DIR, 'train/output_*.csv'))\n",
    "output_df = pd.concat([pd.read_csv(f) for f in output_files], ignore_index=True)\n",
    "print(f\"Loaded {len(output_files)} output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4e4554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:48:51.491460Z",
     "iopub.status.busy": "2025-10-20T00:48:51.491013Z",
     "iopub.status.idle": "2025-10-20T00:48:51.497242Z",
     "shell.execute_reply": "2025-10-20T00:48:51.496016Z",
     "shell.execute_reply.started": "2025-10-20T00:48:51.491434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_catboost_models(model_path):\n",
    "    \"\"\"Load pre-trained CatBoost models\"\"\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    return saved['models_x'], saved['models_y'], saved['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5f10cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:48:51.498994Z",
     "iopub.status.busy": "2025-10-20T00:48:51.498589Z",
     "iopub.status.idle": "2025-10-20T00:48:51.564297Z",
     "shell.execute_reply": "2025-10-20T00:48:51.562751Z",
     "shell.execute_reply.started": "2025-10-20T00:48:51.498953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def engineer_catboost_features(df):\n",
    "    \"\"\"Create physics-based features for CatBoost models\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['velocity_x'] = df['s'] * np.cos(np.radians(df['dir']))\n",
    "    df['velocity_y'] = df['s'] * np.sin(np.radians(df['dir']))\n",
    "    \n",
    "    df['dist_to_ball'] = np.sqrt(\n",
    "        (df['x'] - df['ball_land_x'])**2 + \n",
    "        (df['y'] - df['ball_land_y'])**2\n",
    "    )\n",
    "    \n",
    "    df['angle_to_ball'] = np.arctan2(\n",
    "        df['ball_land_y'] - df['y'],\n",
    "        df['ball_land_x'] - df['x']\n",
    "    )\n",
    "    \n",
    "    df['velocity_toward_ball'] = (\n",
    "        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n",
    "        df['velocity_y'] * np.sin(df['angle_to_ball'])\n",
    "    )\n",
    "    \n",
    "    df['time_to_ball'] = df['num_frames_output'] / 10.0\n",
    "    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n",
    "    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n",
    "    \n",
    "    df['role_targeted_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    df['role_defensive_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    df['role_passer'] = (df['player_role'] == 'Passer').astype(int)\n",
    "    df['side_offense'] = (df['player_side'] == 'Offense').astype(int)\n",
    "    \n",
    "    height_parts = df['player_height'].str.split('-', expand=True)\n",
    "    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
    "    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n",
    "    \n",
    "    df['acceleration_x'] = df['a'] * np.cos(np.radians(df['dir']))\n",
    "    df['acceleration_y'] = df['a'] * np.sin(np.radians(df['dir']))\n",
    "    df['distance_to_target_x'] = df['ball_land_x'] - df['x']\n",
    "    df['distance_to_target_y'] = df['ball_land_y'] - df['y']\n",
    "    df['speed_squared'] = df['s'] ** 2\n",
    "    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n",
    "    df['velocity_alignment'] = np.cos(df['angle_to_ball'] - np.radians(df['dir']))\n",
    "    \n",
    "    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['time_to_ball']\n",
    "    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['time_to_ball']\n",
    "    df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n",
    "    df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n",
    "    df['error_from_ball'] = np.sqrt(df['error_from_ball_x']**2 + df['error_from_ball_y']**2)\n",
    "    \n",
    "    df['momentum_x'] = df['player_weight'] * df['velocity_x']\n",
    "    df['momentum_y'] = df['player_weight'] * df['velocity_y']\n",
    "    df['kinetic_energy'] = 0.5 * df['player_weight'] * df['speed_squared']\n",
    "    \n",
    "    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n",
    "    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n",
    "    \n",
    "    df['time_squared'] = df['time_to_ball'] ** 2\n",
    "    df['dist_squared'] = df['dist_to_ball'] ** 2\n",
    "    df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['time_to_ball'] + 0.1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c646ce6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:13.832514Z",
     "iopub.status.busy": "2025-10-20T00:52:13.830783Z",
     "iopub.status.idle": "2025-10-20T00:52:13.841302Z",
     "shell.execute_reply": "2025-10-20T00:52:13.840133Z",
     "shell.execute_reply.started": "2025-10-20T00:52:13.832464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_sequence_features_catboost(df):\n",
    "    \"\"\"Add temporal features using lag and rolling statistics\"\"\"\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    group_cols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n",
    "    \n",
    "    for window in [3, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_rolling_mean_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
    "                df[f'{col}_rolling_std_{window}'] = df.groupby(group_cols)[col].rolling(window, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
    "    \n",
    "    for col in ['velocity_x', 'velocity_y']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24a423f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:14.541056Z",
     "iopub.status.busy": "2025-10-20T00:52:14.540672Z",
     "iopub.status.idle": "2025-10-20T00:52:14.548770Z",
     "shell.execute_reply": "2025-10-20T00:52:14.547299Z",
     "shell.execute_reply.started": "2025-10-20T00:52:14.541030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_training_dataset(input_df, output_df):\n",
    "    output_df = output_df.copy()\n",
    "    output_df['id'] = (output_df['game_id'].astype(str) + '_' + \n",
    "                    output_df['play_id'].astype(str) + '_' + \n",
    "                    output_df['nfl_id'].astype(str) + '_' + \n",
    "                    output_df['frame_id'].astype(str))\n",
    "    \n",
    "    output_df = output_df.rename(columns={'x': 'target_x', 'y': 'target_y'})\n",
    "    \n",
    "    input_agg = input_df.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n",
    "    \n",
    "    if 'frame_id' in input_agg.columns:\n",
    "        input_agg = input_agg.drop('frame_id', axis=1)\n",
    "    \n",
    "    merged = output_df.merge(\n",
    "        input_agg,\n",
    "        on=['game_id', 'play_id', 'nfl_id'],\n",
    "        how='left',\n",
    "        suffixes=('', '_input')\n",
    "    )\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c83734d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:17.179789Z",
     "iopub.status.busy": "2025-10-20T00:52:17.179414Z",
     "iopub.status.idle": "2025-10-20T00:52:17.187942Z",
     "shell.execute_reply": "2025-10-20T00:52:17.186522Z",
     "shell.execute_reply.started": "2025-10-20T00:52:17.179763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(input_df, features, train_template):\n",
    "    train_features = engineer_catboost_features(input_df)\n",
    "    train_features = add_sequence_features_catboost(train_features)\n",
    "    \n",
    "    train_agg = train_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n",
    "    if 'frame_id' in train_agg.columns:\n",
    "        train_agg = train_agg.drop('frame_id', axis=1)\n",
    "    \n",
    "    train_merged = train_template.merge(\n",
    "        train_agg,\n",
    "        on=['game_id', 'play_id', 'nfl_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    for col in features:\n",
    "        if col not in train_merged.columns:\n",
    "            train_merged[col] = 0\n",
    "    \n",
    "    X_train = train_merged[features].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78dd00fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:17.756607Z",
     "iopub.status.busy": "2025-10-20T00:52:17.756262Z",
     "iopub.status.idle": "2025-10-20T00:52:17.765262Z",
     "shell.execute_reply": "2025-10-20T00:52:17.764141Z",
     "shell.execute_reply.started": "2025-10-20T00:52:17.756583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_catboost(models_x, models_y, features, test_input, test_template):\n",
    "    \"\"\"Generate predictions from CatBoost ensemble\"\"\"\n",
    "    test_features = engineer_catboost_features(test_input)\n",
    "    test_features = add_sequence_features_catboost(test_features)\n",
    "    \n",
    "    test_agg = test_features.groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n",
    "    if 'frame_id' in test_agg.columns:\n",
    "        test_agg = test_agg.drop('frame_id', axis=1)\n",
    "    \n",
    "    test_merged = test_template.merge(\n",
    "        test_agg,\n",
    "        on=['game_id', 'play_id', 'nfl_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    for col in features:\n",
    "        if col not in test_merged.columns:\n",
    "            test_merged[col] = 0\n",
    "    \n",
    "    X_test = test_merged[features].fillna(0).values\n",
    "    \n",
    "    pred_x = np.mean([model.predict(X_test) for model in models_x], axis=0)\n",
    "    pred_y = np.mean([model.predict(X_test) for model in models_y], axis=0)\n",
    "    \n",
    "    predictions = pd.DataFrame({\n",
    "        'id': (test_merged['game_id'].astype(str) + '_' + \n",
    "            test_merged['play_id'].astype(str) + '_' + \n",
    "            test_merged['nfl_id'].astype(str) + '_' + \n",
    "            test_merged['frame_id'].astype(str)),\n",
    "        'x': pred_x,\n",
    "        'y': pred_y\n",
    "    })\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f329c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:19.704236Z",
     "iopub.status.busy": "2025-10-20T00:52:19.703871Z",
     "iopub.status.idle": "2025-10-20T00:52:19.709712Z",
     "shell.execute_reply": "2025-10-20T00:52:19.708676Z",
     "shell.execute_reply.started": "2025-10-20T00:52:19.704212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def height_to_feet(height_str):\n",
    "    try:\n",
    "        ft, inches = map(int, height_str.split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b7c480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:20.228508Z",
     "iopub.status.busy": "2025-10-20T00:52:20.228150Z",
     "iopub.status.idle": "2025-10-20T00:52:20.263249Z",
     "shell.execute_reply": "2025-10-20T00:52:20.262057Z",
     "shell.execute_reply.started": "2025-10-20T00:52:20.228484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_lstm_sequences(input_df, test_template, window_size):\n",
    "    \"\"\"Prepare sequential features for LSTM inference\"\"\"\n",
    "    input_df = input_df.copy()\n",
    "    input_df['player_height_feet'] = input_df['player_height'].map(height_to_feet)\n",
    "    \n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    \n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "    \n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
    "    \n",
    "    from datetime import datetime\n",
    "    current_date = datetime.now()\n",
    "    input_df['age'] = input_df['player_birth_date'].apply(\n",
    "        lambda x: (current_date - datetime.strptime(x, '%Y-%m-%d')).days // 365 if pd.notnull(x) else None\n",
    "    )\n",
    "    \n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "    input_df['force'] = mass_kg * input_df['a']\n",
    "    \n",
    "    input_df['rolling_mean_velocity_x'] = input_df.groupby(['game_id', 'play_id', 'nfl_id'])['velocity_x'].transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=1).mean()\n",
    "    )\n",
    "    input_df['rolling_std_acceleration'] = input_df.groupby(['game_id', 'play_id', 'nfl_id'])['a'].transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=1).std()\n",
    "    )\n",
    "    \n",
    "    if all(col in input_df.columns for col in ['ball_land_x', 'ball_land_y']):\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed'] = (input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "                                     input_df['velocity_y'] * input_df['ball_direction_y'])\n",
    "        input_df['estimated_time_to_ball'] = input_df['distance_to_ball'] / 20.0\n",
    "        input_df['projected_time_to_ball'] = input_df['distance_to_ball'] / (np.abs(input_df['closing_speed']) + 0.1)\n",
    "    \n",
    "    input_df['heading_x'] = np.sin(dir_rad)\n",
    "    input_df['heading_y'] = np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * input_df['heading_x']\n",
    "    input_df['acceleration_y'] = input_df['a'] * input_df['heading_y']\n",
    "    input_df['accel_magnitude'] = np.sqrt(input_df['acceleration_x']**2 + input_df['acceleration_y']**2)\n",
    "    \n",
    "    agg_rows = []\n",
    "    for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "        n = len(grp)\n",
    "        nfl_ids = grp['nfl_id'].to_numpy()\n",
    "        if n < 2:\n",
    "            for nid in nfl_ids:\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': np.nan, 'distance_to_player_min_offense': np.nan, 'distance_to_player_max_offense': np.nan,\n",
    "                    'relative_velocity_magnitude_mean_offense': np.nan, 'relative_velocity_magnitude_min_offense': np.nan, 'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                    'angle_to_player_mean_offense': np.nan, 'angle_to_player_min_offense': np.nan, 'angle_to_player_max_offense': np.nan,\n",
    "                    'distance_to_player_mean_defense': np.nan, 'distance_to_player_min_defense': np.nan, 'distance_to_player_max_defense': np.nan,\n",
    "                    'relative_velocity_magnitude_mean_defense': np.nan, 'relative_velocity_magnitude_min_defense': np.nan, 'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                    'angle_to_player_mean_defense': np.nan, 'angle_to_player_min_defense': np.nan, 'angle_to_player_max_defense': np.nan,\n",
    "                })\n",
    "            continue\n",
    "        x = grp['x'].to_numpy(dtype=np.float32)\n",
    "        y = grp['y'].to_numpy(dtype=np.float32)\n",
    "        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "        is_offense = grp['is_offense'].to_numpy()\n",
    "        is_defense = grp['is_defense'].to_numpy()\n",
    "        dx = x[None, :] - x[:, None]\n",
    "        dy = y[None, :] - y[:, None]\n",
    "        angle_mat = np.arctan2(-dy, -dx)\n",
    "        dist = np.sqrt(dx ** 2 + dy ** 2)\n",
    "        dvx = vx[:, None] - vx[None, :]\n",
    "        dvy = vy[:, None] - vy[None, :]\n",
    "        rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "        offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "        np.fill_diagonal(offense_mask, False)\n",
    "        defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "        np.fill_diagonal(defense_mask, False)\n",
    "        dist_diag_nan = dist.copy()\n",
    "        np.fill_diagonal(dist_diag_nan, np.nan)\n",
    "        rel_diag_nan = rel_speed.copy()\n",
    "        np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "        angle_diag_nan = angle_mat.copy()\n",
    "        np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "        def masked_stats(mat, mask):\n",
    "            masked = np.where(mask, mat, np.nan)\n",
    "            cnt = mask.sum(axis=1)\n",
    "            mean = np.nanmean(masked, axis=1)\n",
    "            amin = np.nanmin(masked, axis=1)\n",
    "            amax = np.nanmax(masked, axis=1)\n",
    "            zero = cnt == 0\n",
    "            mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "            return mean, amin, amax\n",
    "        d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "        v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "        a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "        d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "        v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "        a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "        for idx, nid in enumerate(nfl_ids):\n",
    "            agg_rows.append({\n",
    "                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                'distance_to_player_mean_offense': d_mean_o[idx], 'distance_to_player_min_offense': d_min_o[idx], 'distance_to_player_max_offense': d_max_o[idx],\n",
    "                'relative_velocity_magnitude_mean_offense': v_mean_o[idx], 'relative_velocity_magnitude_min_offense': v_min_o[idx], 'relative_velocity_magnitude_max_offense': v_max_o[idx],\n",
    "                'angle_to_player_mean_offense': a_mean_o[idx], 'angle_to_player_min_offense': a_min_o[idx], 'angle_to_player_max_offense': a_max_o[idx],\n",
    "                'distance_to_player_mean_defense': d_mean_d[idx], 'distance_to_player_min_defense': d_min_d[idx], 'distance_to_player_max_defense': d_max_d[idx],\n",
    "                'relative_velocity_magnitude_mean_defense': v_mean_d[idx], 'relative_velocity_magnitude_min_defense': v_min_d[idx], 'relative_velocity_magnitude_max_defense': v_max_d[idx],\n",
    "                'angle_to_player_mean_defense': a_mean_d[idx], 'angle_to_player_min_defense': a_min_d[idx], 'angle_to_player_max_defense': a_max_d[idx],\n",
    "            })\n",
    "    interaction_agg = pd.DataFrame(agg_rows)\n",
    "    input_df = input_df.merge(interaction_agg, on=['game_id', 'play_id', 'frame_id', 'nfl_id'], how='left')\n",
    "    \n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    \n",
    "    feature_cols = [\n",
    "        'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n",
    "        'absolute_yardline_number',\n",
    "        'player_height_feet','player_weight',\n",
    "        'velocity_x','velocity_y',\n",
    "        'momentum_x','momentum_y',\n",
    "        'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n",
    "        'age','kinetic_energy','force',\n",
    "        'rolling_mean_velocity_x','rolling_std_acceleration',\n",
    "        'heading_x','heading_y','acceleration_x','acceleration_y','accel_magnitude',\n",
    "        'distance_to_ball','angle_to_ball','ball_direction_x','ball_direction_y',\n",
    "        'closing_speed','estimated_time_to_ball','projected_time_to_ball',\n",
    "        'distance_to_ball','angle_to_ball','ball_direction_x','ball_direction_y',\n",
    "        'closing_speed','estimated_time_to_ball','projected_time_to_ball',\n",
    "        'distance_to_player_mean_offense','distance_to_player_min_offense','distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense','relative_velocity_magnitude_min_offense','relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense','angle_to_player_min_offense','angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense','distance_to_player_min_defense','distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense','relative_velocity_magnitude_min_defense','relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense','angle_to_player_min_defense','angle_to_player_max_defense'\n",
    "    ]\n",
    "    \n",
    "    grouped_input = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "    target_groups = test_template[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "    \n",
    "    sequences, sequence_ids = [], []\n",
    "    \n",
    "    for _, row in target_groups.iterrows():\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "        try:\n",
    "            group_df = grouped_input.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        input_window = group_df.tail(window_size)\n",
    "        \n",
    "        if len(input_window) < window_size:\n",
    "            pad_length = window_size - len(input_window)\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_length), columns=input_window.columns)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "        seq = input_window[feature_cols].values\n",
    "        \n",
    "        if np.isnan(seq.astype(np.float32)).any():\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        \n",
    "        last_frame_id = input_window['frame_id'].iloc[-1]\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': last_frame_id\n",
    "        })\n",
    "    \n",
    "    return sequences, sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4998f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:25.840080Z",
     "iopub.status.busy": "2025-10-20T00:52:25.839720Z",
     "iopub.status.idle": "2025-10-20T00:52:25.855734Z",
     "shell.execute_reply": "2025-10-20T00:52:25.854386Z",
     "shell.execute_reply.started": "2025-10-20T00:52:25.840055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_lstm(models_x, models_y, scalers, test_input, test_template):\n",
    "    \"\"\"Generate predictions from LSTM ensemble\"\"\"\n",
    "    sequences, seq_ids = prepare_lstm_sequences(test_input, test_template, Config.LSTM_WINDOW_SIZE)\n",
    "    X_test_unscaled = np.array(sequences, dtype=object)\n",
    "    test_meta = pd.DataFrame(seq_ids)\n",
    "    \n",
    "    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    test_meta['x_last'] = x_last\n",
    "    test_meta['y_last'] = y_last\n",
    "    \n",
    "    per_model_dx, per_model_dy = [], []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for i, (model_x, model_y, scaler) in enumerate(zip(models_x, models_y, scalers)):\n",
    "        scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n",
    "        stacked = np.stack(scaled.astype(np.float32))\n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(stacked))\n",
    "        loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "        \n",
    "        dx_list, dy_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in loader:\n",
    "                batch = batch.to(device)\n",
    "                dx = model_x(batch).cpu().numpy()\n",
    "                dy = model_y(batch).cpu().numpy()\n",
    "                dx_list.append(dx)\n",
    "                dy_list.append(dy)\n",
    "        \n",
    "        dx_cum = np.vstack(dx_list)\n",
    "        dy_cum = np.vstack(dy_list)\n",
    "        per_model_dx.append(dx_cum)\n",
    "        per_model_dy.append(dy_cum)\n",
    "    \n",
    "    ens_dx = np.mean(np.stack(per_model_dx, axis=0), axis=0)\n",
    "    ens_dy = np.mean(np.stack(per_model_dy, axis=0), axis=0)\n",
    "    \n",
    "    out_rows = []\n",
    "    for i, seq_info in test_meta.iterrows():\n",
    "        game_id = int(seq_info['game_id'])\n",
    "        play_id = int(seq_info['play_id'])\n",
    "        nfl_id = int(seq_info['nfl_id'])\n",
    "        \n",
    "        frame_ids = test_template[\n",
    "            (test_template['game_id'] == game_id) &\n",
    "            (test_template['play_id'] == play_id) &\n",
    "            (test_template['nfl_id'] == nfl_id)\n",
    "        ]['frame_id'].sort_values().tolist()\n",
    "        \n",
    "        for t, frame_id in enumerate(frame_ids):\n",
    "            if t < ens_dx.shape[1]:\n",
    "                px = x_last[i] + ens_dx[i, t]\n",
    "                py = y_last[i] + ens_dy[i, t]\n",
    "            else:\n",
    "                px = x_last[i] + ens_dx[i, -1]\n",
    "                py = y_last[i] + ens_dy[i, -1]\n",
    "            \n",
    "            out_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n",
    "                'x': px,\n",
    "                'y': py\n",
    "            })\n",
    "    \n",
    "    predictions = pd.DataFrame(out_rows)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b2e137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:26.299112Z",
     "iopub.status.busy": "2025-10-20T00:52:26.298749Z",
     "iopub.status.idle": "2025-10-20T00:52:26.306045Z",
     "shell.execute_reply": "2025-10-20T00:52:26.304851Z",
     "shell.execute_reply.started": "2025-10-20T00:52:26.299087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_models(model_type):\n",
    "    if model_type == 'catboost':\n",
    "        models = {'model_x': CatBoostRegressor(), 'model_y': CatBoostRegressor()}\n",
    "    elif model_type == 'random_forest':\n",
    "        models = {'model_x': RandomForestRegressor(n_estimators=100, random_state=42), 'model_y': RandomForestRegressor(n_estimators=100, random_state=42)}\n",
    "    elif model_type == 'gradient_boosting':\n",
    "        models = {'model_x': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42), 'model_y': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)}\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5bd1cf3-3dda-4e9d-b3ef-249e75578971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:30.247292Z",
     "iopub.status.busy": "2025-10-20T00:52:30.246914Z",
     "iopub.status.idle": "2025-10-20T00:52:50.610192Z",
     "shell.execute_reply": "2025-10-20T00:52:50.609262Z",
     "shell.execute_reply.started": "2025-10-20T00:52:30.247269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_df = engineer_catboost_features(input_df)\n",
    "train_df = create_training_dataset(input_df, output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36b98e2-7660-4861-89f0-fc11a4039e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:52:50.612255Z",
     "iopub.status.busy": "2025-10-20T00:52:50.611862Z",
     "iopub.status.idle": "2025-10-20T00:52:51.228284Z",
     "shell.execute_reply": "2025-10-20T00:52:51.227037Z",
     "shell.execute_reply.started": "2025-10-20T00:52:50.612174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available features: 43\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "        'x', 'y', 's', 'a', 'o', 'dir',\n",
    "        'velocity_x', 'velocity_y', 'dist_to_ball', 'angle_to_ball',\n",
    "        'velocity_toward_ball', 'time_to_ball', 'orientation_diff',\n",
    "        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer',\n",
    "        'side_offense', 'height_inches', 'player_weight', 'bmi',\n",
    "        'ball_land_x', 'ball_land_y', 'num_frames_output', 'frame_id',\n",
    "        'acceleration_x', 'acceleration_y', 'distance_to_target_x', 'distance_to_target_y',\n",
    "        'speed_squared', 'accel_magnitude', 'velocity_alignment',\n",
    "        'expected_x_at_ball', 'expected_y_at_ball',\n",
    "        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "        'angle_diff', 'time_squared', 'dist_squared', 'weighted_dist_by_time'\n",
    "    ]\n",
    "for lag in [1, 2, 3, 4, 5]:\n",
    "    for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
    "        feature_cols.append(f'{col}_lag{lag}')\n",
    "    \n",
    "for window in [3, 5]:\n",
    "    for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
    "        feature_cols.append(f'{col}_rolling_mean_{window}')\n",
    "        feature_cols.append(f'{col}_rolling_std_{window}')\n",
    "\n",
    "feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n",
    "\n",
    "available_features = [col for col in feature_cols if col in train_df.columns]\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "train_df = train_df.dropna(subset=available_features + ['target_x', 'target_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4b133cf-3621-4da5-a498-d38aedc3262b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:04:12.653506Z",
     "iopub.status.busy": "2025-10-20T02:04:12.653146Z",
     "iopub.status.idle": "2025-10-20T02:04:12.659973Z",
     "shell.execute_reply": "2025-10-20T02:04:12.658666Z",
     "shell.execute_reply.started": "2025-10-20T02:04:12.653482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc1df07d-6d10-48ef-aa4a-f91243f2d334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:09:19.806047Z",
     "iopub.status.busy": "2025-10-20T02:09:19.805559Z",
     "iopub.status.idle": "2025-10-20T02:09:20.876991Z",
     "shell.execute_reply": "2025-10-20T02:09:20.874070Z",
     "shell.execute_reply.started": "2025-10-20T02:09:19.806010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506642, 43) (506642, 2)\n",
      "(56294, 43) (56294, 2)\n"
     ]
    }
   ],
   "source": [
    "X = train_df[available_features].values\n",
    "y = train_df[['target_x', 'target_y']]\n",
    "\n",
    "X_train, X_val, y_train, y_val =train_test_split(X, y, test_size = 0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "y_x_train = y_train['target_x'].values\n",
    "y_y_train = y_train['target_y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4280a7e-d517-44c3-b8d7-6b56a1bc82b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:09:22.933649Z",
     "iopub.status.busy": "2025-10-20T02:09:22.931043Z",
     "iopub.status.idle": "2025-10-20T02:20:38.813248Z",
     "shell.execute_reply": "2025-10-20T02:20:38.811966Z",
     "shell.execute_reply.started": "2025-10-20T02:09:22.933596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_x = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_x.fit(X_train, y_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05e4a9-a2ef-4a2f-a05d-017a0f32e0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:20:38.815058Z",
     "iopub.status.busy": "2025-10-20T02:20:38.814647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_y = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_y.fit(X_train, y_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148934b0-bc13-4e67-a340-11c7f76984c0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_evaluation(y_x_pred, y_y_pred, y_actual):\n",
    "    y_pred = np.stack((y_x_pred, y_y_pred))\n",
    "    result= {}\n",
    "\n",
    "    result['mse'] = mean_squared_error(y_actual, y_pred)\n",
    "    result['mae'] = mean_absolute_error(y_actual, y_pred)\n",
    "    result['msle'] = mean_absolute_error(y_actual, y_pred)\n",
    "    result['mape'] = mean_absolute_percentage_error(y_actual, y_pred)\n",
    "    result['mpd'] = mean_tweedie_deviance(y_actual, y_pred, power=1)\n",
    "    result['mgd'] = mean_tweedie_deviance(y_actual, y_pred, power=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6422e415-7228-411a-be2c-dd49d8db3ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:56:39.648200Z",
     "iopub.status.busy": "2025-10-20T01:56:39.647797Z",
     "iopub.status.idle": "2025-10-20T01:56:48.745179Z",
     "shell.execute_reply": "2025-10-20T01:56:48.743573Z",
     "shell.execute_reply.started": "2025-10-20T01:56:39.648173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "catboost_pred = predict_catboost(model_x, model_y, available_features, test_input, test_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c477677c-5217-472d-b426-a93c9596083f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:00:52.049104Z",
     "iopub.status.busy": "2025-10-20T02:00:52.048752Z",
     "iopub.status.idle": "2025-10-20T02:00:52.097082Z",
     "shell.execute_reply": "2025-10-20T02:00:52.094868Z",
     "shell.execute_reply.started": "2025-10-20T02:00:52.049082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catboost_pred.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92990734",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     print(\"Loading test data...\")\n",
    "#     test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "#     test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "#     print(f\"Test input: {test_input.shape[0]:,} rows, {test_input.shape[1]} columns\")\n",
    "#     print(f\"Test template: {test_template.shape[0]:,} predictions required\")\n",
    "    \n",
    "#     print(\"\\nCreate models...\")\n",
    "#     models_x_cat, models_y_cat, features_cat = load_catboost_models(Config.CATBOOST_MODEL_PATH)\n",
    "#     print(f\"CatBoost: {len(models_x_cat)} folds, {len(features_cat)} features\")\n",
    "    \n",
    "#     print(\"\\nGenerating CatBoost predictions...\")\n",
    "#     catboost_pred = predict_catboost(models_x_cat, models_y_cat, features_cat, test_input, test_template)\n",
    "#     print(f\"CatBoost predictions: {len(catboost_pred):,}\")\n",
    "#     print(f\"  X range: [{catboost_pred['x'].min():.2f}, {catboost_pred['x'].max():.2f}]\")\n",
    "#     print(f\"  Y range: [{catboost_pred['y'].min():.2f}, {catboost_pred['y'].max():.2f}]\")\n",
    "    \n",
    "#     print(\"\\nGenerating LSTM predictions...\")\n",
    "#     lstm_pred = predict_lstm(models_x_lstm, models_y_lstm, scalers_lstm, test_input, test_template)\n",
    "#     print(f\"LSTM predictions: {len(lstm_pred):,}\")\n",
    "#     print(f\"  X range: [{lstm_pred['x'].min():.2f}, {lstm_pred['x'].max():.2f}]\")\n",
    "#     print(f\"  Y range: [{lstm_pred['y'].min():.2f}, {lstm_pred['y'].max():.2f}]\")\n",
    "    \n",
    "#     print(\"\\nCreating ensemble...\")\n",
    "#     print(f\"\\nEnsemble predictions: {len(catboost_pred):,}\")\n",
    "#     print(f\"  X range: [{ensemble_pred['x'].min():.2f}, {ensemble_pred['x'].max():.2f}]\")\n",
    "#     print(f\"  Y range: [{ensemble_pred['y'].min():.2f}, {ensemble_pred['y'].max():.2f}]\")\n",
    "#     print(f\"  Mean X: {ensemble_pred['x'].mean():.2f}, Std X: {ensemble_pred['x'].std():.2f}\")\n",
    "#     print(f\"  Mean Y: {ensemble_pred['y'].mean():.2f}, Std Y: {ensemble_pred['y'].std():.2f}\")\n",
    "#     print(f\"  NaN values: {ensemble_pred.isnull().sum().sum()}\")\n",
    "    \n",
    "#     ensemble_pred.to_csv('submission.csv', index=False)\n",
    "#     print(\"\\nSubmission file created successfully\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa3ebf",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_x = CatBoostRegressor(\n",
    "#             iterations=1000,\n",
    "#             learning_rate=0.05,\n",
    "#             depth=10,\n",
    "#             l2_leaf_reg=3.0,\n",
    "#             random_seed=42, \n",
    "#             task_type='GPU',\n",
    "#             devices='0',\n",
    "#             early_stopping_rounds=500,\n",
    "#             verbose=200,\n",
    "#             loss_function='RMSE'\n",
    "#         )\n",
    "        \n",
    "# model_x.fit(\n",
    "#             X,\n",
    "#             eval_set=y_x,\n",
    "#             verbose=200\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13825858,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
